apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: elasticsearch-cluster # Name of your Elasticsearch cluster
  namespace: elastic-data    # Deploy into the 'elastic-data' namespace
spec:
  version: "8.13.4" # IMPORTANT: Verify this version is compatible with your ECK operator and meets your needs.
  auth: {} # Enables default authentication. ECK will generate credentials.
  http:
    tls:
      selfSignedCertificate:
        disabled: false # Uses self-signed certificates for HTTP by default. Consider your own for production.
  nodeSets:
  - name: master # Dedicated master nodes
    count: 3
    config:
      node.roles: ["master", "remote_cluster_client"]
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources: # Starting resources for master nodes. Monitor and adjust.
            requests:
              memory: "4Gi"
              cpu: "1"
            limits:
              memory: "4Gi"
              cpu: "2"

  - name: data # Dedicated data nodes
    count: 3    # For 3TB primary data + 1 replica = 6TB total. (6TB / 3 nodes = 2TB per node)
    config:
      node.roles: ["data", "ingest", "ml", "transform"] # Common roles for data nodes
      node.store.allow_mmap: false # CRITICAL: Due to GKE Autopilot restrictions.
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources: # Starting resources for data nodes. Monitor performance and costs, then adjust.
            requests:
              memory: "16Gi"
              cpu: "2"
            limits:
              memory: "16Gi" # Consider if your GKE Autopilot quotas/limits allow this per pod.
              cpu: "4"
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data # ECK requires this specific name for the data volume.
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: "2Ti" # 2 Terabytes per data node. This is a significant amount of storage.
                           # Double-check costs and that your GKE project quotas allow this.
        # storageClassName: "premium-rwo" # Optional: For GKE SSD performance (higher cost).
                                         # If commented out, the GKE cluster's default StorageClass will be used (usually 'standard-rwo').